"""
LangGraph LLMãƒãƒ¼ãƒ‰çµ±åˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€LangGraphã«LLMï¼ˆChatOpenAIï¼‰ã‚’çµ±åˆã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ï¼š
- LLMã‚’ãƒãƒ¼ãƒ‰ã¨ã—ã¦çµ„ã¿è¾¼ã‚€
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ´»ç”¨
- è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

å®Ÿè¡Œæ–¹æ³•:
python examples/05_llm_node.py

æ³¨æ„: OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™
"""

import os
from typing import TypedDict, List
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage


# ã‚¹ãƒ†ãƒƒãƒ—1: çŠ¶æ…‹ã®å®šç¾©
class LLMGraphState(TypedDict):
    """
    LLMã‚°ãƒ©ãƒ•ã§ä½¿ç”¨ã™ã‚‹çŠ¶æ…‹ã‚’å®šç¾©
    
    Attributes:
        messages: ä¼šè©±å±¥æ­´
        user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
        ai_response: AIã®å¿œç­”
        context: è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        step_count: å®Ÿè¡Œã—ãŸã‚¹ãƒ†ãƒƒãƒ—æ•°
    """
    messages: List[str]
    user_input: str
    ai_response: str
    context: str
    step_count: int


# ã‚¹ãƒ†ãƒƒãƒ—2: LLMã®è¨­å®š
def setup_llm():
    """
    OpenAI LLMã‚’è¨­å®š
    
    Returns:
        è¨­å®šæ¸ˆã¿ã®ChatOpenAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    # APIã‚­ãƒ¼ã®ç¢ºèª
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError(
            "OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
            "ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã™ã‚‹ã‹ã€\n"
            "os.environ['OPENAI_API_KEY'] = 'your-key' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )
    
    # ChatOpenAIã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",  # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        temperature=0.7,        # å¿œç­”ã®å‰µé€ æ€§ï¼ˆ0-2ï¼‰
        max_tokens=500,         # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    )
    
    print("âœ… LLMãŒæ­£å¸¸ã«è¨­å®šã•ã‚Œã¾ã—ãŸ")
    return llm


# ã‚¹ãƒ†ãƒƒãƒ—3: LLMãƒãƒ¼ãƒ‰ã®å®šç¾©
def input_processing_node(state: LLMGraphState) -> LLMGraphState:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å‡¦ç†ã™ã‚‹ãƒãƒ¼ãƒ‰
    
    Args:
        state: ç¾åœ¨ã®çŠ¶æ…‹
        
    Returns:
        æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """
    print("ğŸ“ input_processing_nodeãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸ")
    
    user_input = state.get("user_input", "")
    
    # å…¥åŠ›ã®å‰å‡¦ç†
    processed_input = user_input.strip()
    if not processed_input:
        processed_input = "ã“ã‚“ã«ã¡ã¯ï¼"
    
    print(f"   å‡¦ç†ã•ã‚ŒãŸå…¥åŠ›: {processed_input}")
    
    return {
        **state,
        "user_input": processed_input,
        "step_count": state.get("step_count", 0) + 1
    }


def llm_response_node(state: LLMGraphState) -> LLMGraphState:
    """
    LLMã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ãƒãƒ¼ãƒ‰
    
    Args:
        state: ç¾åœ¨ã®çŠ¶æ…‹
        
    Returns:
        æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """
    print("ğŸ¤– llm_response_nodeãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸ")
    
    try:
        # LLMã‚’è¨­å®š
        llm = setup_llm()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
        system_message = SystemMessage(content="""
ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã‚ã‹ã‚Šã‚„ã™ãä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
å›ç­”ã¯æ—¥æœ¬èªã§è¡Œã„ã€ç°¡æ½”ã§ã‚ã‚ŠãªãŒã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
""")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
        user_message = HumanMessage(content=state["user_input"])
        
        # LLMã«é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
        messages = [system_message, user_message]
        
        print(f"   LLMã«é€ä¿¡: {state['user_input']}")
        
        # LLMã‹ã‚‰å¿œç­”ã‚’å–å¾—
        response = llm.invoke(messages)
        ai_response = response.content
        
        print(f"   LLMã‹ã‚‰ã®å¿œç­”: {ai_response[:100]}...")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’æ›´æ–°
        updated_messages = state.get("messages", [])
        updated_messages.extend([
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {state['user_input']}",
            f"AI: {ai_response}"
        ])
        
        return {
            **state,
            "ai_response": ai_response,
            "messages": updated_messages,
            "step_count": state["step_count"] + 1
        }
        
    except Exception as e:
        print(f"âŒ LLMãƒãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        error_response = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        
        return {
            **state,
            "ai_response": error_response,
            "step_count": state["step_count"] + 1
        }


def context_enhancement_node(state: LLMGraphState) -> LLMGraphState:
    """
    å¿œç­”ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã™ã‚‹ãƒãƒ¼ãƒ‰
    
    Args:
        state: ç¾åœ¨ã®çŠ¶æ…‹
        
    Returns:
        æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """
    print("ğŸ” context_enhancement_nodeãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸ")
    
    ai_response = state.get("ai_response", "")
    step_count = state.get("step_count", 0)
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¿½åŠ 
    enhanced_context = f"""
--- å¿œç­”æƒ…å ± ---
å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—æ•°: {step_count}
å¿œç­”æ–‡å­—æ•°: {len(ai_response)}
å‡¦ç†æ™‚åˆ»: LangGraphã§å‡¦ç†æ¸ˆã¿

å…ƒã®å¿œç­”:
{ai_response}
"""
    
    print(f"   ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    
    return {
        **state,
        "context": enhanced_context,
        "step_count": step_count + 1
    }


# ã‚¹ãƒ†ãƒƒãƒ—4: é«˜åº¦ãªLLMãƒãƒ¼ãƒ‰ - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
def advanced_llm_node(state: LLMGraphState) -> LLMGraphState:
    """
    é«˜åº¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹LLMãƒãƒ¼ãƒ‰
    
    Args:
        state: ç¾åœ¨ã®çŠ¶æ…‹
        
    Returns:
        æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """
    print("ğŸš€ advanced_llm_nodeãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸ")
    
    try:
        llm = setup_llm()
        user_input = state["user_input"]
        
        # é«˜åº¦ãªã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """
ã‚ãªãŸã¯å°‚é–€çš„ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

1. **è¦ç´„**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ä¸€æ–‡ã§è¦ç´„
2. **å›ç­”**: è©³ç´°ã§å®Ÿç”¨çš„ãªå›ç­”
3. **è£œè¶³**: é–¢é€£ã™ã‚‹è¿½åŠ æƒ…å ±ã‚„ææ¡ˆ
4. **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«å–ã‚‹ã¹ãè¡Œå‹•

å›ç­”ã¯æ§‹é€ åŒ–ã•ã‚Œã€å®Ÿè¡Œå¯èƒ½ã§ä¾¡å€¤ã®ã‚ã‚‹å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚
"""
        
        # ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        conversation_history = "\n".join(state.get("messages", [])[-4:])  # æœ€æ–°4ã¤ã®å±¥æ­´
        
        enhanced_prompt = f"""
{system_prompt}

ä¼šè©±å±¥æ­´:
{conversation_history}

ç¾åœ¨ã®è³ªå•: {user_input}
"""
        
        messages = [
            SystemMessage(content=enhanced_prompt),
            HumanMessage(content=user_input)
        ]
        
        response = llm.invoke(messages)
        advanced_response = response.content
        
        print(f"   é«˜åº¦ãªå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        
        return {
            **state,
            "ai_response": advanced_response,
            "step_count": state["step_count"] + 1
        }
        
    except Exception as e:
        print(f"âŒ é«˜åº¦ãªLLMãƒãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            **state,
            "ai_response": f"é«˜åº¦ãªå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}",
            "step_count": state["step_count"] + 1
        }


# ã‚¹ãƒ†ãƒƒãƒ—5: ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
def create_llm_graph():
    """
    LLMã‚’å«ã‚€ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
    
    Returns:
        ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã®ã‚°ãƒ©ãƒ•
    """
    print("ğŸ“Š LLMã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ä¸­...")
    
    workflow = StateGraph(LLMGraphState)
    
    # ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
    workflow.add_node("input_processing", input_processing_node)
    workflow.add_node("llm_response", llm_response_node)
    workflow.add_node("context_enhancement", context_enhancement_node)
    
    # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
    workflow.add_edge("input_processing", "llm_response")
    workflow.add_edge("llm_response", "context_enhancement")
    workflow.add_edge("context_enhancement", END)
    
    # é–‹å§‹ç‚¹ã‚’è¨­å®š
    workflow.set_entry_point("input_processing")
    
    return workflow.compile()


def create_advanced_llm_graph():
    """
    é«˜åº¦ãªLLMã‚°ãƒ©ãƒ•ã‚’ä½œæˆï¼ˆæ¡ä»¶åˆ†å²ä»˜ãï¼‰
    
    Returns:
        ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã®ã‚°ãƒ©ãƒ•
    """
    print("ğŸ“Š é«˜åº¦ãªLLMã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ä¸­...")
    
    def decide_llm_type(state: LLMGraphState) -> str:
        """ã©ã®LLMãƒãƒ¼ãƒ‰ã‚’ä½¿ã†ã‹ã‚’æ±ºå®š"""
        user_input = state.get("user_input", "")
        
        # è³ªå•ã®è¤‡é›‘ã•ã§åˆ¤æ–­
        if any(keyword in user_input.lower() for keyword in ["è©³ã—ã", "èª¬æ˜", "æ•™ãˆã¦", "æ–¹æ³•"]):
            print("   â†’ è©³ç´°ãªè³ªå•ãªã®ã§é«˜åº¦ãªLLMã‚’ä½¿ç”¨")
            return "advanced_llm"
        else:
            print("   â†’ ç°¡å˜ãªè³ªå•ãªã®ã§æ¨™æº–LLMã‚’ä½¿ç”¨")
            return "standard_llm"
    
    workflow = StateGraph(LLMGraphState)
    
    # ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
    workflow.add_node("input_processing", input_processing_node)
    workflow.add_node("standard_llm", llm_response_node)
    workflow.add_node("advanced_llm", advanced_llm_node)
    workflow.add_node("context_enhancement", context_enhancement_node)
    
    # æ¡ä»¶åˆ†å²ã‚¨ãƒƒã‚¸
    workflow.add_conditional_edges(
        "input_processing",
        decide_llm_type,
        {
            "standard_llm": "standard_llm",
            "advanced_llm": "advanced_llm"
        }
    )
    
    # é€šå¸¸ã®ã‚¨ãƒƒã‚¸
    workflow.add_edge("standard_llm", "context_enhancement")
    workflow.add_edge("advanced_llm", "context_enhancement")
    workflow.add_edge("context_enhancement", END)
    
    workflow.set_entry_point("input_processing")
    
    return workflow.compile()


# ã‚¹ãƒ†ãƒƒãƒ—6: å®Ÿè¡Œä¾‹
def run_llm_example():
    """
    LLMã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œä¾‹
    """
    print("=" * 60)
    print("ğŸ¤– LangGraph + LLM å®Ÿè¡Œé–‹å§‹")
    print("=" * 60)
    
    try:
        # åŸºæœ¬ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        graph = create_llm_graph()
        
        # ãƒ†ã‚¹ãƒˆè³ªå•ãƒªã‚¹ãƒˆ
        test_questions = [
            "ã“ã‚“ã«ã¡ã¯ï¼LangGraphã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "Pythonã§ãƒªã‚¹ãƒˆã‚’ä½œã‚‹æ–¹æ³•ã¯ï¼Ÿ",
            "ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
            ""  # ç©ºã®å…¥åŠ›ãƒ†ã‚¹ãƒˆ
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ {i}: {question if question else '(ç©ºã®å…¥åŠ›)'}")
            print("-" * 40)
            
            # åˆæœŸçŠ¶æ…‹
            initial_state = {
                "messages": [],
                "user_input": question,
                "ai_response": "",
                "context": "",
                "step_count": 0
            }
            
            # ã‚°ãƒ©ãƒ•å®Ÿè¡Œ
            result = graph.invoke(initial_state)
            
            print(f"\nâœ… AIå¿œç­”:")
            print(result["ai_response"])
            print(f"\nã‚¹ãƒ†ãƒƒãƒ—æ•°: {result['step_count']}")
            
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")


def run_advanced_llm_example():
    """
    é«˜åº¦ãªLLMã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œä¾‹
    """
    print("\n" + "=" * 60)
    print("ğŸš€ é«˜åº¦ãªLLMã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œ")
    print("=" * 60)
    
    try:
        graph = create_advanced_llm_graph()
        
        # ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è³ªå•ã‚’ãƒ†ã‚¹ãƒˆ
        questions = [
            "ã“ã‚“ã«ã¡ã¯",  # ã‚·ãƒ³ãƒ—ãƒ«ãªæŒ¨æ‹¶
            "æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„"  # è¤‡é›‘ãªè³ªå•
        ]
        
        for question in questions:
            print(f"\nğŸ“ è³ªå•: {question}")
            print("-" * 40)
            
            result = graph.invoke({
                "messages": [],
                "user_input": question,
                "ai_response": "",
                "context": "",
                "step_count": 0
            })
            
            print(f"âœ… AIå¿œç­”:")
            print(result["ai_response"])
            
    except Exception as e:
        print(f"âŒ é«˜åº¦ãªå®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼: {e}")


def interactive_mode():
    """
    ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
    """
    print("\n" + "=" * 60)
    print("ğŸ’¬ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    print("'quit'ã¾ãŸã¯'çµ‚äº†'ã§çµ‚äº†")
    print("=" * 60)
    
    try:
        graph = create_advanced_llm_graph()
        messages = []
        
        while True:
            user_input = input("\nè³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
            
            if user_input.lower() in ['quit', 'çµ‚äº†', 'exit']:
                print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            
            if not user_input:
                print("âŒ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                continue
            
            try:
                result = graph.invoke({
                    "messages": messages,
                    "user_input": user_input,
                    "ai_response": "",
                    "context": "",
                    "step_count": 0
                })
                
                print(f"\nğŸ¤– AI: {result['ai_response']}")
                messages = result["messages"]
                
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    # APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ä»¥ä¸‹ã®æ–¹æ³•ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
        print("1. ç’°å¢ƒå¤‰æ•°: export OPENAI_API_KEY='your-key'")
        print("2. ã‚³ãƒ¼ãƒ‰å†…: os.environ['OPENAI_API_KEY'] = 'your-key'")
        print("\nè¨­å®šå¾Œã€å†åº¦å®Ÿè¡Œã—ã¦ãã ã•ã„")
        exit(1)
    
    # åŸºæœ¬ä¾‹ã®å®Ÿè¡Œ
    run_llm_example()
    
    # é«˜åº¦ãªä¾‹ã®å®Ÿè¡Œ
    run_advanced_llm_example()
    
    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã®ææ¡ˆ
    print("\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("   - examples/06_stateful_agent.py ã§çŠ¶æ…‹ç®¡ç†ã‚’å­¦ç¿’")
    print("   - interactive_mode()ã‚’å‘¼ã³å‡ºã—ã¦ãƒãƒ£ãƒƒãƒˆã‚’ä½“é¨“")
    print("   - ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„")
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã®å®Ÿè¡Œ
    # interactive_mode()  # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤ã§å®Ÿè¡Œ
